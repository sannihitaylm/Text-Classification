{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c4bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ba55054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148f6995f66c4c27ab1979335dc56a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c551fc677aa45a68727ea5a7230887c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a91601c3fb4b8d90381e7c5431bd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36232e8292d449d184c26d7824537eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cabee77b4a4045911de0353e12125a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafc75a9258c4a39827116d226addec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # Set timeout to 10 minutes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbff4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b74cdb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watermarked Text: Prompt: What are the benefits of artificial intelligence? Share your thoughts below.\n",
      "\n",
      "When it comes to artificial intelligence, the technology is not just a buzzword. It’s a real thing, and it’s here to stay. As AI gets better and better, and as more companies and government agencies experiment with it, there are growing concerns about what the future holds.\n",
      "\n",
      "The U.S. government has its own big AI ambitions, with a $100 billion AI research initiative planned for 2018, and the U.S. military is getting into\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Model and tokenizer names\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define watermark text and prompt\n",
    "watermark_text = \" Share your thoughts below.\"\n",
    "prompt_text = \"Prompt: What are the benefits of artificial intelligence?\"\n",
    "\n",
    "# Tokenize watermark and prompt\n",
    "watermark_tokens = tokenizer(watermark_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "prompt_tokens = tokenizer(prompt_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Combine prompt and watermark tokens with separator token [SEP]\n",
    "combined_tokens = torch.cat([prompt_tokens, watermark_tokens], dim=-1)\n",
    "\n",
    "# Generate watermarked text by passing the combined tokens through the model\n",
    "watermarked_output = model.generate(\n",
    "    input_ids=combined_tokens,\n",
    "    max_length=combined_tokens.shape[-1] + 100,  # Adjust as needed\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Convert token IDs back to text\n",
    "watermarked_text = tokenizer.decode(watermarked_output[0], skip_special_tokens=True)\n",
    "print(\"Watermarked Text:\", watermarked_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6a391a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watermarked Text: Prompt: What is AI an its uses?  Share your thoughts below.\n",
      "\n",
      "I am a computer scientist who studies the question of “what is AI?”. The question is a question of philosophy, which means that it is a question of what is real and what is illusory, what is the objective reality and what is the subjective reality. The answer to the question is that the objective reality is the reality that is known to the human observer, which is known to the human observer by being known to the human observer by the things that the human\n",
      "Perplexity Score: 45172.23\n",
      "Watermarked Text is Machine-Written\n"
     ]
    }
   ],
   "source": [
    "#Related with perplexity \n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define watermark text and prompt\n",
    "watermark_text = \" Share your thoughts below.\"\n",
    "prompt_text = \"Prompt: What is AI an its uses? \"\n",
    "\n",
    "# Tokenize watermark and prompt\n",
    "watermark_tokens = tokenizer(watermark_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "prompt_tokens = tokenizer(prompt_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Combine prompt and watermark tokens with separator token [SEP]\n",
    "combined_tokens = torch.cat([prompt_tokens, watermark_tokens], dim=-1)\n",
    "\n",
    "# Generate watermarked text by passing the combined tokens through the model\n",
    "watermarked_output = model.generate(\n",
    "    input_ids=combined_tokens,\n",
    "    max_length=combined_tokens.shape[-1] + 100,  \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Convert token IDs back to text\n",
    "watermarked_text = tokenizer.decode(watermarked_output[0], skip_special_tokens=True)\n",
    "print(\"Watermarked Text:\", watermarked_text)\n",
    "\n",
    "# Calculate perplexity score manually\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids=combined_tokens).logits\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    target_ids = watermarked_output[0][:combined_tokens.size(1)]  \n",
    "    perplexity_score = torch.exp(-log_probs[0, range(target_ids.size(0)), target_ids]).mean()\n",
    "print(f\"Perplexity Score: {perplexity_score:.2f}\")\n",
    "\n",
    "# Set a threshold for detection \n",
    "perplexity_threshold = 1000\n",
    "\n",
    "if perplexity_score <= perplexity_threshold:\n",
    "    print(\"Watermarked Text is Human-Written\")\n",
    "else:\n",
    "    print(\"Watermarked Text is Machine-Written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "978cfb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watermarked Text: Prompt: What are the benefits of artificial intelligence?Share your thoughts below.\n",
      "\n",
      "You may think that the answer to the question \"What is Artificial Intelligence?\" is obvious. After all, if you can create a computer that is intelligent, surely it must be possible to create a computer that is not intelligent.\n",
      "\n",
      "That's not entirely true. AI is not a new field of study. The field of artificial intelligence (AI) has been around since the 1930s. In fact, the field was created in the late 1950s by the mathematician John McCarthy, who was\n",
      "Watermark Detected: Watermarked Text is Machine-Generated\n"
     ]
    }
   ],
   "source": [
    "#Related with no perplexity \n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Model and tokenizer names\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define watermark text and prompt\n",
    "watermark_text = \"Share your thoughts below.\"\n",
    "prompt_text = \"Prompt: What are the benefits of artificial intelligence?\"\n",
    "\n",
    "# Tokenize watermark and prompt\n",
    "watermark_tokens = tokenizer(watermark_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "prompt_tokens = tokenizer(prompt_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Combine prompt and watermark tokens with separator token [SEP]\n",
    "combined_tokens = torch.cat([prompt_tokens, watermark_tokens], dim=-1)\n",
    "\n",
    "# Generate watermarked text by passing the combined tokens through the model\n",
    "watermarked_output = model.generate(\n",
    "    input_ids=combined_tokens,\n",
    "    max_length=combined_tokens.shape[-1] + 100,  # Adjust as needed\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Convert token IDs back to text\n",
    "watermarked_text = tokenizer.decode(watermarked_output[0], skip_special_tokens=True)\n",
    "print(\"Watermarked Text:\", watermarked_text)\n",
    "\n",
    "# Detecting the watermark\n",
    "if watermark_text in watermarked_text:\n",
    "    print(\"Watermark Detected: Watermarked Text is Machine-Generated\")\n",
    "else:\n",
    "    print(\"Watermark Not Detected: Watermarked Text is Human-Written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8945c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watermarked Text: Prompt: What is AI and its uses?  Share your thoughts below.\n",
      "\n",
      "“AI” is short for Artificial Intelligence. It is the use of computers to\n",
      "learn, perform, and solve complex tasks. It is the ability to think and\n",
      "act like a human. It is a combination of artificial intelligence and\n",
      "biological intelligence.\n",
      "\n",
      "In the context of the internet, it is usually referred to as “artificial\n",
      "intelligence”. In this blog post, we will discuss how AI works and what\n",
      "is its role in our society\n"
     ]
    }
   ],
   "source": [
    "#Part 1 for watermarking and Part 2 for identifying between human or machine written\n",
    "#Part 1\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define watermark text and prompt\n",
    "watermark_text = \" Share your thoughts below.\"\n",
    "prompt_text = \"Prompt: What is AI and its uses? \"\n",
    "\n",
    "# Tokenize watermark and prompt\n",
    "watermark_tokens = tokenizer(watermark_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "prompt_tokens = tokenizer(prompt_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Combine prompt and watermark tokens with separator token [SEP]\n",
    "combined_tokens = torch.cat([prompt_tokens, watermark_tokens], dim=-1)\n",
    "\n",
    "# Generate watermarked text by passing the combined tokens through the model\n",
    "watermarked_output = model.generate(\n",
    "    input_ids=combined_tokens,\n",
    "    max_length=combined_tokens.shape[-1] + 100,  \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Convert token IDs back to text\n",
    "watermarked_text = tokenizer.decode(watermarked_output[0], skip_special_tokens=True)\n",
    "print(\"Watermarked Text:\", watermarked_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b2e712a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       455\n",
      "           1       0.93      0.98      0.96       347\n",
      "\n",
      "    accuracy                           0.96       802\n",
      "   macro avg       0.96      0.96      0.96       802\n",
      "weighted avg       0.96      0.96      0.96       802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 2 detection using dataset/machine learning model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/Users/sannihitayalamanchili/Desktop/data.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# columns\n",
    "data['Headline'] = data['Headline'].fillna('')\n",
    "data['Body'] = data['Body'].fillna('')\n",
    "\n",
    "# body and label columns\n",
    "X = data['Body']\n",
    "y = data['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use TF-IDF for feature extraction.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression model.\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions.\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model.\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9652a322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Machine-Written\n"
     ]
    }
   ],
   "source": [
    "#Part 2 detection perplexity based detection\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the text you want to classify\n",
    "text_to_classify = \"Hi whats up, how is it going with you?\"\n",
    "\n",
    "# Tokenize the text\n",
    "input_ids = tokenizer(text_to_classify, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Calculate perplexity score manually\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids=input_ids).logits\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    target_ids = input_ids.squeeze(0)\n",
    "    perplexity_score = torch.exp(-log_probs[0, range(target_ids.size(0)), target_ids]).mean()\n",
    "    \n",
    "# Set a threshold for detection \n",
    "perplexity_threshold = 2000\n",
    "if perplexity_score <= perplexity_threshold:\n",
    "    print(\"Classification: Human-Written\")\n",
    "else:\n",
    "    print(\"Classification: Machine-Written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9dce8735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/38/ae/0a6b73156176c10ff52b94f5444712bcdb8d22dddf68f106c14f0937e390/openai-1.2.4-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.2.4-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/82/61/a5fca4a1e88e40969bbd0cf0d981f3aa76d5057db160b94f49603fc18740/httpx-0.25.1-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.25.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Obtaining dependency information for pydantic<3,>=1.9.0 from https://files.pythonhosted.org/packages/d7/10/ddfb9539a6e55f7dfd6c2b9b81d86fcba2761ba87eeb81f8b1012957dcdc/pydantic-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.5.0-py3-none-any.whl.metadata (174 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.6/174.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>4 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for httpcore from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for pydantic-core==2.14.1 from https://files.pythonhosted.org/packages/a1/83/716b8dd7529d9c8487b103068bbe12313a2eda7ff609077d006e82a286b8/pydantic_core-2.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pydantic_core-2.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.2.4-py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.2/220.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.5.0-py3-none-any.whl (407 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.5/407.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.1-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydantic-core, h11, distro, annotated-types, pydantic, httpcore, httpx, openai\n",
      "Successfully installed annotated-types-0.6.0 distro-1.8.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.2.4 pydantic-2.5.0 pydantic-core-2.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d6d50e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: openai in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (1.2.4)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from openai) (0.25.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from openai) (2.5.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.1 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c0fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: openai 1.2.4\r\n",
      "Uninstalling openai-1.2.4:\r\n",
      "  Would remove:\r\n",
      "    /Users/sannihitayalamanchili/anaconda3/bin/openai\r\n",
      "    /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages/openai-1.2.4.dist-info/*\r\n",
      "    /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages/openai/*\r\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "!pip uninstall openai\n",
    "!pip install openai==0.27.0\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41865c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: transformers in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (4.31.0)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/9a/06/e4ec2a321e57c03b7e9345d709d554a52c33760e5015fdff0919d9459af0/transformers-4.35.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/65/cc/2891260847777eb9aaca278aaf3f846c9ff8ea1351643a4f33ff26d5d213/huggingface_hub-0.19.1-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/7e/8e/9c0f7799da9a690ec29a7a7b6c0744d3f735e40951d2f62c8202faf3df6a/tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sannihitayalamanchili/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.31.0\n",
      "    Uninstalling transformers-4.31.0:\n",
      "      Successfully uninstalled transformers-4.31.0\n",
      "Successfully installed huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.35.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce113219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c06fe94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f8e7abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with Human Code: 0.0\n",
      "Similarity with Model-Generated: 0.1875\n",
      "This is model-generated.\n"
     ]
    }
   ],
   "source": [
    "#test with Jaccard similarity (machine input)\n",
    "human_written_code = [\n",
    "    \"Hi, how's it going?\",\n",
    "    \"Have you finished your homework?\",\n",
    "]\n",
    "\n",
    "model_generated_code = [\n",
    "    \"for i in range(20):\",\n",
    "    \"print(i * 2)\",\n",
    "]\n",
    "\n",
    "# input \n",
    "user_code = [\n",
    "    \"for number in numbers: result = number * 2\",\n",
    "    \"printf Double of {number}\",\n",
    "]\n",
    "\n",
    "# Function to compute Jaccard similarity\n",
    "def jaccard_similarity(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "user_vs_human = jaccard_similarity(' '.join(user_code), ' '.join(human_written_code))\n",
    "user_vs_model = jaccard_similarity(' '.join(user_code), ' '.join(model_generated_code))\n",
    "\n",
    "print(\"Similarity with Human Code:\", user_vs_human)\n",
    "print(\"Similarity with Model-Generated:\", user_vs_model)\n",
    "\n",
    "if user_vs_human > user_vs_model:\n",
    "    print(\"This is human-written.\")\n",
    "else:\n",
    "    print(\"This is model-generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55cee668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with Human Code: 0.26666666666666666\n",
      "Similarity with Model-Generated Code: 0.0\n",
      "This is human-written.\n"
     ]
    }
   ],
   "source": [
    "#test with Jaccard similarity human input\n",
    "human_written_code = [\n",
    "    \"Hi, how's it going?\",\n",
    "    \"Have you finished your homework?\",\n",
    "]\n",
    "\n",
    "model_generated_code = [\n",
    "    \"for i in range(20):\",\n",
    "    \"print(i * 2)\",\n",
    "]\n",
    "\n",
    "# input\n",
    "user_code = [\n",
    "    \"hey bro how is it going?\",\n",
    "    \"how have you been nowadays?\",\n",
    "]\n",
    "\n",
    "# Function to compute Jaccard similarity\n",
    "def jaccard_similarity(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "user_vs_human = jaccard_similarity(' '.join(user_code), ' '.join(human_written_code))\n",
    "user_vs_model = jaccard_similarity(' '.join(user_code), ' '.join(model_generated_code))\n",
    "\n",
    "print(\"Similarity with Human Code:\", user_vs_human)\n",
    "print(\"Similarity with Model-Generated Code:\", user_vs_model)\n",
    "\n",
    "if user_vs_human > user_vs_model:\n",
    "    print(\"This is human-written.\")\n",
    "else:\n",
    "    print(\"This is model-generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "365639a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is human-written code.\n"
     ]
    }
   ],
   "source": [
    "#test with cosine similarity human input\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# human written\n",
    "human_written_code = [\n",
    "    \"Hi, how's it going?\",\n",
    "    \"Have you finished your homework?\"\n",
    "]\n",
    "#machine written\n",
    "model_generated_code = [\n",
    "    \"for i in range(10):\",\n",
    "    \"    print(i * 2)\"\n",
    "]\n",
    "\n",
    "# input\n",
    "user_code = [\n",
    "    \"hey bro how is it going?\",\n",
    "    \"how have you been nowadays?\"\n",
    "]\n",
    "\n",
    "# vectorize using TF-IDF(Term Frequency-Inverse Document Frequency) \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(user_code + human_written_code + model_generated_code)\n",
    "\n",
    "#  calc cosine similarities\n",
    "user_vs_human = np.mean(cosine_similarity(tfidf_matrix[:len(user_code)], tfidf_matrix[len(user_code):len(user_code)+len(human_written_code)]))\n",
    "user_vs_model = np.mean(cosine_similarity(tfidf_matrix[:len(user_code)], tfidf_matrix[len(user_code)+len(human_written_code):]))\n",
    "\n",
    "if user_vs_human > user_vs_model:\n",
    "    print(\"This is human-written code.\")\n",
    "else:\n",
    "    print(\"This is model-generated code.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acaabedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is model-generated code.\n"
     ]
    }
   ],
   "source": [
    "#test with Cosine similarity machine input\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# human written\n",
    "human_written_code = [\n",
    "    \"Hi, how's it going?\",\n",
    "    \"Have you finished your homework?\"\n",
    "]\n",
    "\n",
    "#machine written\n",
    "model_generated_code = [\n",
    "    \"for i in range(10):\",\n",
    "    \"    print(i * 2)\"\n",
    "]\n",
    "\n",
    "# input\n",
    "user_code = [\n",
    "    \"for i in range(10):\",\n",
    "    \"    print(i * 3)\"\n",
    "]\n",
    "\n",
    "# vectorize using TF-IDF(Term Frequency-Inverse Document Frequency) \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(user_code + human_written_code + model_generated_code)\n",
    "\n",
    "#  calc cosine similarities\n",
    "user_vs_human = np.mean(cosine_similarity(tfidf_matrix[:len(user_code)], tfidf_matrix[len(user_code):len(user_code)+len(human_written_code)]))\n",
    "user_vs_model = np.mean(cosine_similarity(tfidf_matrix[:len(user_code)], tfidf_matrix[len(user_code)+len(human_written_code):]))\n",
    "\n",
    "if user_vs_human > user_vs_model:\n",
    "    print(\"This is human-written code.\")\n",
    "else:\n",
    "    print(\"This is model-generated code.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d0a8b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is LLM-generated text.\n"
     ]
    }
   ],
   "source": [
    "# using reference dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Example human-written text\n",
    "human_written_text = [\n",
    "    \"This is an example of human-written text.\",\n",
    "    \"Another sample sentence written by a human.\",\n",
    "]\n",
    "\n",
    "# Example LLM-generated text\n",
    "model_generated_text = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"An LLM generated this text with a prompt.\",\n",
    "]\n",
    "\n",
    "# input\n",
    "user_text = [\n",
    "    \"A user provides this text for comparison.\",\n",
    "    \"It is up to the program to determine its origin.\",\n",
    "]\n",
    "\n",
    "# Reference dataset (Wikipedia articles)\n",
    "reference_text = [\n",
    "    \"Wikipedia is a free online encyclopedia with articles written by humans.\",\n",
    "    \"It covers a wide range of topics and is a good source of human-written text.\",\n",
    "    \"Choose a suitable reference dataset for your specific use case.\",\n",
    "]\n",
    "\n",
    "# vectorize using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(user_text + human_written_text + model_generated_text + reference_text)\n",
    "\n",
    "# Calc cosine similarities\n",
    "user_vs_human = np.mean(cosine_similarity(tfidf_matrix[:len(user_text)], tfidf_matrix[len(user_text):(len(user_text) + len(human_written_text))]))\n",
    "user_vs_model = np.mean(cosine_similarity(tfidf_matrix[:len(user_text)], tfidf_matrix[(len(user_text) + len(human_written_text)):(len(user_text) + len(human_written_text) + len(model_generated_text))]))\n",
    "\n",
    "if user_vs_human > user_vs_model:\n",
    "    print(\"This is human-written text.\")\n",
    "else:\n",
    "    print(\"This is LLM-generated text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44b4b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is human-written text.\n"
     ]
    }
   ],
   "source": [
    "# using stylometric features\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Example human-written text\n",
    "human_written_text = [\n",
    "    \"This is an example of human-written text.\",\n",
    "    \"Another sample sentence written by a human.\",\n",
    "]\n",
    "\n",
    "# Example LLM-generated text\n",
    "model_generated_text = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"An LLM generated this text with a prompt.\",\n",
    "]\n",
    "\n",
    "# input\n",
    "user_text = [\n",
    "    \"Hi, hows it is going with you?\",\n",
    "    \"This is an example of how you should do the homework\",\n",
    "]\n",
    "\n",
    "# Feature extraction w stylometric features\n",
    "vectorizer = TfidfVectorizer(analyzer='word', max_features=1000, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(user_text + human_written_text + model_generated_text)\n",
    "\n",
    "# Calc cosine similarities\n",
    "user_vs_human = np.mean(cosine_similarity(tfidf_matrix[:len(user_text)], tfidf_matrix[len(user_text):(len(user_text) + len(human_written_text))]))\n",
    "user_vs_model = np.mean(cosine_similarity(tfidf_matrix[:len(user_text)], tfidf_matrix[(len(user_text) + len(human_written_text)):(len(user_text) + len(human_written_text) + len(model_generated_text))]))\n",
    "\n",
    "if user_vs_human > user_vs_model:\n",
    "    print(\"This is human-written text.\")\n",
    "else:\n",
    "    print(\"This is LLM-generated text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ceae6983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       455\n",
      "           1       0.93      0.98      0.96       347\n",
      "\n",
      "    accuracy                           0.96       802\n",
      "   macro avg       0.96      0.96      0.96       802\n",
      "weighted avg       0.96      0.96      0.96       802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 2 using dataset/machine learning model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/Users/sannihitayalamanchili/Desktop/data.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# columns\n",
    "data['Headline'] = data['Headline'].fillna('')\n",
    "data['Body'] = data['Body'].fillna('')\n",
    "\n",
    "# body and label columns\n",
    "X = data['Body']\n",
    "y = data['Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use TF-IDF for feature extraction.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression model.\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions.\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model.\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f82db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "757497e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m chatgpt_tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     87\u001b[0m text_to_classify \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the realm of quantum computing, entanglement serves as a fundamental phenomenon that enables the creation of qubits with unparalleled computational capabilities. Harnessing the power of superposition and quantum parallelism, these systems can perform intricate calculations at speeds previously deemed impossible by classical computing architectures. The implications for cryptography, optimization problems, and simulations are profound, marking a paradigm shift in our approach to complex problem-solving.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 88\u001b[0m detect_human_or_ai(text_to_classify, chatgpt_model, chatgpt_tokenizer)\n",
      "Cell \u001b[0;32mIn[39], line 69\u001b[0m, in \u001b[0;36mdetect_human_or_ai\u001b[0;34m(text, model, tokenizer, perplexity_threshold, burstiness_threshold)\u001b[0m\n\u001b[1;32m     66\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m calculate_perplexity(text, model, tokenizer)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Simulate ChatGPT API responses using OpenAI API\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m chatgpt_outputs \u001b[38;5;241m=\u001b[39m simulate_chatgpt_api(text, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Train a surrogate model using GPT-2\u001b[39;00m\n\u001b[1;32m     72\u001b[0m surrogate_model \u001b[38;5;241m=\u001b[39m train_surrogate_model(chatgpt_outputs)\n",
      "Cell \u001b[0;32mIn[39], line 21\u001b[0m, in \u001b[0;36msimulate_chatgpt_api\u001b[0;34m(prompt, num_samples)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimulate_chatgpt_api\u001b[39m(prompt, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Use the OpenAI API to generate responses\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     22\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-002\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# or any other available engine\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     24\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m     25\u001b[0m         n\u001b[38;5;241m=\u001b[39mnum_samples,\n\u001b[1;32m     26\u001b[0m         stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Extract and return the generated responses\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     generated_responses \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    613\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    616\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    620\u001b[0m             result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    621\u001b[0m             result\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    622\u001b[0m             result\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    623\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    624\u001b[0m         ),\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:679\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    677\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    680\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    681\u001b[0m     )\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "# Perplexity based detection(Improvised) with API key not working\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set your OpenAI API key\n",
    "#openai.api_key = ''\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model(input_ids, labels=input_ids).loss\n",
    "    return torch.exp(loss)\n",
    "\n",
    "# Function to simulate ChatGPT API responses using OpenAI API\n",
    "def simulate_chatgpt_api(prompt, num_samples=100):\n",
    "    # Use the OpenAI API to generate responses\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",  # or any other available engine\n",
    "        prompt=prompt,\n",
    "        max_tokens=150,\n",
    "        n=num_samples,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Extract and return the generated responses\n",
    "    generated_responses = [item['text'].strip() for item in response['choices']]\n",
    "    return generated_responses\n",
    "\n",
    "# Function to train a surrogate model using GPT-2\n",
    "def train_surrogate_model(data):\n",
    "    # Train a simple surrogate model using linear regression\n",
    "    responses = data\n",
    "    response_lengths = [len(response.split()) for response in responses]\n",
    "    \n",
    "    # Features: Use response lengths as input to the surrogate model\n",
    "    X = np.array(response_lengths).reshape(-1, 1)\n",
    "    \n",
    "    # Labels: Use indices of responses as output labels\n",
    "    y = np.arange(len(responses)).reshape(-1, 1)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train a linear regression model\n",
    "    surrogate_model = LinearRegression()\n",
    "    surrogate_model.fit(X_train, y_train)\n",
    "    \n",
    "    return surrogate_model\n",
    "\n",
    "# Function to calculate burstiness\n",
    "def calculate_burstiness(text, generated_responses):\n",
    "    # Calculate burstiness based on the length of the provided responses\n",
    "    response_lengths = [len(response.split()) for response in generated_responses]\n",
    "    mean_length = np.mean(response_lengths)\n",
    "    burstiness = np.std(response_lengths) / mean_length\n",
    "    return burstiness\n",
    "\n",
    "# Function to detect if a given text is human or AI\n",
    "def detect_human_or_ai(text, model, tokenizer, perplexity_threshold=20, burstiness_threshold=0.2):\n",
    "    # Calculate perplexity\n",
    "    perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "    \n",
    "    # Simulate ChatGPT API responses using OpenAI API\n",
    "    chatgpt_outputs = simulate_chatgpt_api(text, num_samples=100)\n",
    "    \n",
    "    # Train a surrogate model using GPT-2\n",
    "    surrogate_model = train_surrogate_model(chatgpt_outputs)\n",
    "    \n",
    "    # Calculate burstiness\n",
    "    burstiness = calculate_burstiness(text, chatgpt_outputs)\n",
    "    \n",
    "    # Classify as human or AI based on perplexity and burstiness\n",
    "    if perplexity < perplexity_threshold and burstiness < burstiness_threshold:\n",
    "        print(f\"The response is classified as human (perplexity: {perplexity:.2f}, burstiness: {burstiness:.2f})\")\n",
    "    else:\n",
    "        print(f\"The response is classified as AI (perplexity: {perplexity:.2f}, burstiness: {burstiness:.2f})\")\n",
    "\n",
    "# Example usage\n",
    "model_name = \"gpt2\"\n",
    "chatgpt_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "chatgpt_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "text_to_classify = \"In the realm of quantum computing, entanglement serves as a fundamental phenomenon that enables the creation of qubits with unparalleled computational capabilities. Harnessing the power of superposition and quantum parallelism, these systems can perform intricate calculations at speeds previously deemed impossible by classical computing architectures. The implications for cryptography, optimization problems, and simulations are profound, marking a paradigm shift in our approach to complex problem-solving.\"\n",
    "detect_human_or_ai(text_to_classify, chatgpt_model, chatgpt_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39ce6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response is classified as AI (perplexity: 25.34, burstiness: 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Perplexity based detection(Improvised) machine input\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Calculate perplexity\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model(input_ids, labels=input_ids).loss\n",
    "    return torch.exp(loss)\n",
    "\n",
    "# Simulate ChatGPT API responses\n",
    "def simulate_chatgpt_api(num_samples=100):\n",
    "    generated_responses = [\"This is a simulated response.\" for _ in range(num_samples)]\n",
    "    return generated_responses\n",
    "\n",
    "# Function to train a surrogate model using GPT-2\n",
    "def train_surrogate_model(data):\n",
    "    # Train a simple surrogate model using linear regression\n",
    "    responses = data\n",
    "    response_lengths = [len(response.split()) for response in responses]\n",
    "    \n",
    "    # Features: Using response lengths as input to the surrogate model\n",
    "    X = np.array(response_lengths).reshape(-1, 1)\n",
    "    \n",
    "    # Labels: Use indices of responses as output labels\n",
    "    y = np.arange(len(responses)).reshape(-1, 1)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train a linear regression model\n",
    "    surrogate_model = LinearRegression()\n",
    "    surrogate_model.fit(X_train, y_train)\n",
    "    \n",
    "    return surrogate_model\n",
    "\n",
    "# Calculate burstiness\n",
    "def calculate_burstiness(text, generated_responses):\n",
    "    response_lengths = [len(response.split()) for response in generated_responses]\n",
    "    mean_length = np.mean(response_lengths)\n",
    "    burstiness = np.std(response_lengths) / mean_length\n",
    "    return burstiness\n",
    "\n",
    "# Function to detect if a given text is human or AI\n",
    "def detect_human_or_ai(text, model, tokenizer, perplexity_threshold=20, burstiness_threshold=0.2):\n",
    "    # Calc perplexity\n",
    "    perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "    \n",
    "    # Simulate ChatGPT API responses\n",
    "    chatgpt_outputs = simulate_chatgpt_api(num_samples=100)\n",
    "    \n",
    "    # Train a surrogate model using GPT-2\n",
    "    surrogate_model = train_surrogate_model(chatgpt_outputs)\n",
    "    \n",
    "    # Calcu burstiness\n",
    "    burstiness = calculate_burstiness(text, chatgpt_outputs)\n",
    "    \n",
    "    # Classify as human or AI based on perplexity and burstiness\n",
    "    if perplexity < perplexity_threshold and burstiness < burstiness_threshold:\n",
    "        print(f\"The response is classified as human (perplexity: {perplexity:.2f}, burstiness: {burstiness:.2f})\")\n",
    "    else:\n",
    "        print(f\"The response is classified as AI (perplexity: {perplexity:.2f}, burstiness: {burstiness:.2f})\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model_name = \"gpt2\"\n",
    "chatgpt_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "chatgpt_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "text_to_classify = \"In the realm of quantum computing, entanglement serves as a fundamental phenomenon that enables the creation of qubits with unparalleled computational capabilities. Harnessing the power of superposition and quantum parallelism, these systems can perform intricate calculations at speeds previously deemed impossible by classical computing architectures. The implications for cryptography, optimization problems, and simulations are profound, marking a paradigm shift in our approach to complex problem-solving.\"\n",
    "detect_human_or_ai(text_to_classify, chatgpt_model, chatgpt_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8831d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response is classified as human (perplexity: 14.34, burstiness: 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Perplexity based detection(Improvised) human input\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# calculate perplexity\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model(input_ids, labels=input_ids).loss\n",
    "    return torch.exp(loss)\n",
    "\n",
    "# simulate ChatGPT API responses\n",
    "def simulate_chatgpt_api(num_samples=100):\n",
    "    # Placeholder: Generate random responses for simulation\n",
    "    generated_responses = [\"This is a simulated response.\" for _ in range(num_samples)]\n",
    "    return generated_responses\n",
    "\n",
    "# Train a surrogate model using GPT-2\n",
    "def train_surrogate_model(data):\n",
    "    # Train a simple surrogate model using linear regression\n",
    "    responses = data\n",
    "    response_lengths = [len(response.split()) for response in responses]\n",
    "    \n",
    "    # Features: Use response lengths as input to the surrogate model\n",
    "    X = np.array(response_lengths).reshape(-1, 1)\n",
    "    \n",
    "    # Labels: Use indices of responses as output labels\n",
    "    y = np.arange(len(responses)).reshape(-1, 1)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train a linear regression model\n",
    "    surrogate_model = LinearRegression()\n",
    "    surrogate_model.fit(X_train, y_train)\n",
    "    \n",
    "    return surrogate_model\n",
    "\n",
    "# calculate burstiness\n",
    "def calculate_burstiness(text, generated_responses):\n",
    "    # Calculate burstiness based on the length of the provided responses\n",
    "    response_lengths = [len(response.split()) for response in generated_responses]\n",
    "    mean_length = np.mean(response_lengths)\n",
    "    burstiness = np.std(response_lengths) / mean_length\n",
    "    return burstiness\n",
    "\n",
    "# Function to detect if a given text is human or AI\n",
    "def detect_human_or_ai(text, model, tokenizer, perplexity_threshold=20, burstiness_threshold=0.2):\n",
    "    # Calc perplexity\n",
    "    perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "    \n",
    "    # Simulate ChatGPT API responses\n",
    "    chatgpt_outputs = simulate_chatgpt_api(num_samples=100)\n",
    "    \n",
    "    # Train a surrogate model using GPT-2\n",
    "    surrogate_model = train_surrogate_model(chatgpt_outputs)\n",
    "    \n",
    "    # Calc burstiness\n",
    "    burstiness = calculate_burstiness(text, chatgpt_outputs)\n",
    "    \n",
    "    # Classify as human or AI based on perplexity and burstiness\n",
    "    if perplexity < perplexity_threshold and burstiness < burstiness_threshold:\n",
    "        print(f\"The response is classified as human (perplexity: {perplexity:.2f}, burstiness: {burstiness:.2f})\")\n",
    "    else:\n",
    "        print(f\"The response is classified as AI (perplexity: {perplexity:.2f}, burstiness: {burstiness:.2f})\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model_name = \"gpt2\"\n",
    "chatgpt_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "chatgpt_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "text_to_classify = \"Hi, how are you today?\"\n",
    "detect_human_or_ai(text_to_classify, chatgpt_model, chatgpt_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da23eb-3816-4943-b990-0ce576344a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 for watermarking and Part 2 for identifying between human or machine written\n",
    "#Part 1\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define watermark text and prompt\n",
    "watermark_text = \" Share your thoughts below.\"\n",
    "prompt_text = \"Prompt: What is AI and its uses? \"\n",
    "\n",
    "# Tokenize watermark and prompt\n",
    "watermark_tokens = tokenizer(watermark_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "prompt_tokens = tokenizer(prompt_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Combine prompt and watermark tokens with separator token [SEP]\n",
    "combined_tokens = torch.cat([prompt_tokens, watermark_tokens], dim=-1)\n",
    "\n",
    "# Generate watermarked text by passing the combined tokens through the model\n",
    "watermarked_output = model.generate(\n",
    "    input_ids=combined_tokens,\n",
    "    max_length=combined_tokens.shape[-1] + 100,  \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Convert token IDs back to text\n",
    "watermarked_text = tokenizer.decode(watermarked_output[0], skip_special_tokens=True)\n",
    "print(\"Watermarked Text:\", watermarked_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ea836-4174-4b56-bc8f-f1d38048ba87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
